---
title: "R Notebook"
output: html_notebook
---

# Feature selection

Feature selection is one of the most important tasks to boost performance of machine learning models. Some of the benefits of doing feature selections include:

- Better Accuracy: removing irrelevant features let the models make decisions only using important features. In my experience, classification models can usually get 5 to 10 percent improvement in accuracy scores after feature selection.

- Avoid Overfitting: the models will not put weights in irrelevant features.

- Improve running time: decreasing the dimension of the data makes the models run faster.

Required packages
```{r}
library(ggcorrplot)
library(caret)
library(randomForest)
library(gam)
```

# Differential expression genes

Loading data

```{r}
# load the data
deg_data <- read.table("data/top500DEGs.txt", header=TRUE, sep="\t", as.is=c(1,2))
names(deg_data) = chartr(".", "-", names(deg_data))
deg_data = data.frame(t(deg_data))
deg_data
```

```{r}
clinical_data <- read.table("data/clinical_data_354s.txt", header=TRUE, sep="\t", as.is=c(1,2))
clinical_data
```

## Correlation Matrix

Correlation matrix is a popular method for feature selection. By using correlation matrix, we can see the correlation for each pair of numerical variables. we not only can filter out variables with low correlation to the dependent variable, but also can remove redundant variables by identifying highly correlated independent variables.

```{r}
cor_matrix <- data.frame(cor(deg_data))
names(cor_matrix) = chartr(".", "-", names(cor_matrix))
cor_matrix
```
### Heatmap for correlation matrix

We can also draw a quick heat map to visualize the correlation with ggcorrplot library.

```{r fig3, fig.height = 25, fig.width = 25, fig.align = "center"}
ggcorrplot(cor_matrix)
```
We can see that genes OLAH, PCSK1 and RCOR2 have very small correlations with the dependent variable quality. We may consider to try removing these two features in the models.


### Function to find redundant variables

Besides from using graph to identify redundant variables, we can also use the findCorrelation function in caret library. It outputs the index of variables we need to delete. If two variables have a higher correlation than the cutoff, the function removes the variable with the largest mean absolute correlation.

In our case, the function finds out that density is highly correlated to other variables.

```{r}
findCorrelation(cor(deg_data), cutoff=0.75)
```

## Variable Importance

There are some functions in caret library to evaluate variable importance. Typically, variable importance evaluation can be separated into two categories: the ones that use model information and the ones that do not.

### Calculate feature importance without models

If we don’t want to use specific model to evaluate features, we can use the filterVarImp() function in Caret library.

For classification, this function uses ROC curve analysis on each predictor and use area under the curve as scores.

For regression, a linear line is fit between each pair of dependent variable and independent variables, and the absolute value of the t-statistic for the slope of the independent variable is used.

?? The data set is a regression task, so t-statistic of slopes are used. The three variables OLAH, PCSK1 and RCOR2 have very low scores, which are consistent with our correlation analysis above.

```{r}
#use roc_curve area as score
roc_imp <- filterVarImp(x = deg_data, y = clinical_data$histological_type)

#sort the score in decreasing order
roc_imp <- data.frame(cbind(variable = rownames(roc_imp), score = roc_imp[,1]))
roc_imp$score <- as.double(roc_imp$score)
roc_imp[order(roc_imp$score,decreasing = TRUE),]
```

### Calculate feature importance with models

The advantage for using model-specific evaluation is that the feature scores are specific linked to model performance and using them to filter features may help improve performance for that specific model.

There are some models that do not have built-in importance score. For those models, we need to use the filterVarImp() function above. To check which models are available, see documentation of varImp(): https://www.documentation.org/packages/caret/versions/6.0-90/topics/varImp

We train a random forest regression model for the data set and use the varImp() function to calculate the feature importance based on this tree model. In the case of random forest model, varLmp() is a wrapper around the importance functions from the randomForest library.

```{r}
#train random forest model and calculate feature importance
rf = randomForest(x = deg_data,y = clinical_data$histological_type)
var_imp <- varImp(rf, scale = FALSE)
```

```{r}
#sort the score in decreasing order
var_imp_df <- data.frame(cbind(variable = rownames(var_imp), score = var_imp[,1]))
var_imp_df$score <- as.double(var_imp_df$score)
var_imp_df[order(var_imp_df$score,decreasing = TRUE),]
```


We can see that the ranking here is different from the previous one and they are specifically linked to the performance of the random forest model.

```{r fig4, fig.height = 60, fig.width = 25, fig.align = "center"}
library(ggplot2)
ggplot(var_imp_df, aes(x=reorder(variable, score), y=score)) + 
  geom_point() +
  geom_segment(aes(x=variable,xend=variable,y=0,yend=score)) +
  ylab("IncNodePurity") +
  xlab("Variable Name") +
  coord_flip()
```

## Univariate Feature Selection

Univariate tests are tests which involve only one dependent variable, including chi-sqaure test, analysis of variance, linear regressions and t-tests of means.

Univariate feature selection utilizes univariate statistical tests to each feature-outcome pair and selects features which perform the best in these tests.

The sbf() is used to do univariate feature selection with the model fitting function specified in the function argument of sbfControl() function. In the following example, I uses rfSBF, which use node purity to calculate scores.

To avoid overfitting by feature selection, sbf() function use several iterations of resampling to do feature selection. resampling methods can be boot, cv, LOOCV or LGOCV and can be specified in the method argument of sbfControl().

It will take several minutes to run.

```{r}
filterCtrl <- sbfControl(functions = rfSBF, method = "repeatedcv", repeats = 3)
rfWithFilter <- sbf(x = deg_data, y = clinical_data$histological_type, sbfControl = filterCtrl)
rfWithFilter
```

## Recursive Feature elimination

Recursive Feature elimination(RFE) is a popular method to choose a subset of features. It starts with all the features and removes feature with lowest score at each iteration. It trains with smaller and smaller subset of features and find the best set of features.

It will also take several minutes to run.

```{r}
filterCtrl <- rfeControl(functions=rfFuncs, method="cv", number=3)
results <- rfe(x = deg_data,y = clinical_data$histological_type, sizes=c(seq.int(1, 500 , by = 1)), rfeControl=filterCtrl)
results
```

```{r}
# plot the results
plot(results, type=c("g", "o"))
```

```{r}
print(paste("Optimal feature size - ", results$optsize))
```
```{r}
results$optVariables
```



# Machine learning

#Support vector machine

For DESeqDataSet object raw read counts are needed
```{r}
raw_data = read.table("data/filtered/RNA_RAW_FeatureCounts_filtered.txt", header=TRUE, sep="\t", as.is=c(1,2))
names(raw_data) = chartr(".", "-", names(raw_data))
raw_data
```
Let's generate data with selected features, which will be further used in machine learning
```{r}
names.use <- rownames(raw_data)[rownames(raw_data) %in% results$optVariables]
raw_data.selected_features <- raw_data[rownames(raw_data) %in% names.use, ]
raw_data.selected_features
```


Divide data set on training and test sets
```{r}
set.seed(1)
nTest <- ceiling(ncol(raw_data.selected_features) * 0.2)
ind <- sample(ncol(raw_data.selected_features), nTest, FALSE)
```

```{r}
# Minimum count is set to 1 in order to prevent 0 division problem within
# classification models.
data.train <- as.matrix(raw_data.selected_features[ ,-ind] + 1)
data.test <- as.matrix(raw_data.selected_features[ ,ind] + 1)
classtr <- data.frame(condition = clinical_data$histological_type[-ind])
classts <- data.frame(condition = clinical_data$histological_type[ind])
```

The training and test sets are stored in a DESeqDataSet using related functions from DESeq2. This object is then used as input for MLSeq.
```{r}
library (DESeq2)
data.trainS4 = DESeqDataSetFromMatrix(countData = data.train, colData = classtr,
design = formula(~condition))
data.testS4 = DESeqDataSetFromMatrix(countData = data.test, colData = classts,
design = formula(~condition))
```

## Optimizing model parameters

The MLSeq has a single function classify for the model building and evaluation process. This function can be used to evaluate selected classifier using a set of values for model parameter (aka tuning parameter ) and return the optimal model. The overall model performances for training set are also returned.

MLSeq evaluates k-fold repeated cross-validation on training set for selecting the optimal value of tuning parameter. The number of parameters to be optimized depends on the selected classifier. Some classifiers have two or more tuning parameter, while some have no tuning parameter. Suppose we want to fit RNA-Seq counts to Support Vector Machines with Radial Basis Function Kernel (svmRadial) using deseq normalization and vst transformation


```{r}
library(MLSeq)
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
                preProcessing = "deseq-vst", ref = "Infiltrating_Lobular_Carcinoma",
                tuneLength = 10,
                control = trainControl(method = "repeatedcv", number = 5,
                                      repeats = 10, classProbs = TRUE))
show(fit.svm)
```
The model were trained using 5-fold cross validation repeated 10 times. The number of levels for tuning parameter is set to 10. The length of tuning parameter space, tuneLength, may be increased to be more sensitive while searching optimal value of the parameters. However, this may drastically increase the total computation time. The tuning results are obtained using setter function trained as:

```{r}
trained(fit.svm)
```
The optimal values for tuning parameters were sigma = 0.00721 and C = 4. The effect of tuning
parameters on model accuracies can be graphically seen in Figure:

```{r}
plot(fit.svm)
```
Figure: Tuning results for fitted model (svmRadial)

The C hyperparameter tells the SVM optimization how much we want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, we should get misclassified examples, often even if your training data is linearly separable.

## Defining control list for selected classifier

For each classifier, it is possible to define how model should be created using control lists. We may categorize available classifiers into 3 partitions, i.e continuous, discrete and voom-based classifiers. Continuous classifiers are based on caret’s library while discrete and voom-based classifiers use functions from MLSeq’slibrary. Since each classifier category has different control parameters to be used while building model, we should use corresponding control function for selected classifiers. 

Now, we fit svmRadial, voomDLDA and PLDA classifiers to RNA-seq data and find the optimal value of tuning parameters, if available, using 5-fold cross validation without repeats. We may control model building process using related function for the selected classifier

```{r}
# Define control list
ctrl.svm <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
ctrl.plda <- discreteControl(method = "repeatedcv", number = 5, repeats = 1,
                            tuneLength = 10)
ctrl.voomDLDA <- voomControl(method = "repeatedcv", number = 5, repeats = 1,
                            tuneLength = 10)

# Support vector machines with radial basis function kernel
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
                    preProcessing = "deseq-vst", ref = "Infiltrating_Lobular_Carcinoma", 
                    tuneLength = 10, control = ctrl.svm)

# Poisson linear discriminant analysis
fit.plda <- classify(data = data.trainS4, method = "PLDA", normalize = "deseq",
                    ref = "Infiltrating_Lobular_Carcinoma", control = ctrl.plda)

# Voom-based diagonal linear discriminant analysis
fit.voomDLDA <- classify(data = data.trainS4, method = "voomDLDA",
                        normalize = "deseq", ref = "Infiltrating_Lobular_Carcinoma", 
                        control = ctrl.voomDLDA)

```
```{r}
trained(fit.voomDLDA)
```
```{r}
trained(fit.plda)
```

```{r}
trained(fit.svm)
```
## Predicting the class labels of test samples

Class labels of the test cases are predicted based on the model characteristics of the trained model, e.g. discriminating function of the trained model in discriminant-based classifiers. However, an important point here is that the test set must have passed the same steps with the training set. This is especially true for the normalization and transformation stages for RNA-Seq based classification studies.

MLSeq predicts test samples using training set parameters.

```{r}
pred.svm <- predict(fit.svm, data.testS4)
pred.svm
```
Finally, the model performance for the prediction is summarized as below using confusionMatrix from caret.

```{r}
pred.svm <- relevel(pred.svm, ref = "Infiltrating_Lobular_Carcinoma")
actual <- relevel(classts$condition, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.svm, Actual = actual)
confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

```



```{r}
set.seed(1)
# Define control lists.
ctrl.continuous <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
ctrl.discrete <- discreteControl(method = "repeatedcv", number = 5, repeats = 10,
                                tuneLength = 10)
ctrl.voom <- voomControl(method = "repeatedcv", number = 5, repeats = 10,
                        tuneLength = 10)
# 1. Continuous classifiers, SVM and NSC
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
                    preProcessing = "deseq-vst", ref = "Infiltrating_Lobular_Carcinoma", 
                    tuneLength = 10,
                    control = ctrl.continuous)
fit.NSC <- classify(data = data.trainS4, method = "pam",
                    preProcessing = "deseq-vst", ref = "Infiltrating_Lobular_Carcinoma", 
                    tuneLength = 10,
                    control = ctrl.continuous)

# 2. Discrete classifiers
fit.plda <- classify(data = data.trainS4, method = "PLDA", normalize = "deseq",
                     ref = "Infiltrating_Lobular_Carcinoma", 
                     control = ctrl.discrete)
fit.plda2 <- classify(data = data.trainS4, method = "PLDA2", normalize = "deseq",
                      ref = "Infiltrating_Lobular_Carcinoma", 
                      control = ctrl.discrete)
fit.nblda <- classify(data = data.trainS4, method = "NBLDA", normalize = "deseq",
                      ref = "Infiltrating_Lobular_Carcinoma", 
                      control = ctrl.discrete)
# 3. voom-based classifiers
fit.voomDLDA <- classify(data = data.trainS4, method = "voomDLDA",
                         normalize = "deseq", ref = "Infiltrating_Lobular_Carcinoma", 
                         control = ctrl.voom)
fit.voomNSC <- classify(data = data.trainS4, method = "voomNSC",
                        normalize = "deseq", ref = "Infiltrating_Lobular_Carcinoma", 
                        control = ctrl.voom)
# 4. Predictions
pred.svm <- predict(fit.svm, data.testS4)
pred.NSC <- predict(fit.NSC, data.testS4)
pred.plda <- predict(fit.plda, data.testS4)
pred.plda2 <- predict(fit.plda2, data.testS4)
pred.nblda <- predict(fit.nblda, data.testS4)
pred.voomDLDA <- predict(fit.voomDLDA, data.testS4)
pred.voomNSC <- predict(fit.voomNSC, data.testS4)
```

```{r}
actual <- relevel(classts$condition, ref = "Infiltrating_Lobular_Carcinoma")
#  1. Continuous classifiers, SVM and NSC
pred.svm <- relevel(pred.svm, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.svm, Actual = actual)
cm_svm = confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

pred.NSC <- relevel(pred.NSC, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.NSC, Actual = actual)
cm_NSC = confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

# 2. Discrete classifiers
pred.plda <- relevel(pred.plda, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.plda, Actual = actual)
cm_plda = confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

pred.plda2 <- relevel(pred.plda2, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.plda2, Actual = actual)
cm_plda2 = confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

pred.nblda <- relevel(pred.nblda, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.nblda, Actual = actual)
cm_nblda = confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

# 3. voom-based classifiers
pred.voomDLDA <- relevel(pred.voomDLDA, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.voomDLDA, Actual = actual)
cm_voomDLDA = confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

pred.voomNSC <- relevel(pred.voomNSC, ref = "Infiltrating_Lobular_Carcinoma")
tbl <- table(Predicted = pred.voomNSC, Actual = actual)
cm_voomNSC = confusionMatrix(tbl, positive = "Infiltrating_Lobular_Carcinoma")

classification_results = data.frame(classifier = c("SVM", "NSC", "PLDA", "PLDA2",
                                    "NBLDA", "voomDLDA", "voomNSC"),
                                    accuracy = c(cm_svm$overall['Accuracy'], cm_NSC$overall['Accuracy'],
                                                 cm_plda$overall['Accuracy'], cm_plda2$overall['Accuracy'], 
                                                 cm_nblda$overall['Accuracy'], cm_voomDLDA$overall['Accuracy'],
                                                 cm_voomNSC$overall['Accuracy']))
classification_results
```
The best accuracy is by PLDA2

## Determining possible biomarkers using sparse classifiers

In an RNA-Seq study, hundreds or thousands of features are able to be sequenced for a specific disease or condition. However, not all features but usually a small subset of sequenced features might be differentially expressed among classes and contribute to discrimination function. Hence, determining differentially expressed (DE) features are one of main purposes in an RNA-Seq study. It is possible to select DE features using sparse algorithm in MLSeq such as NSC, PLDA and voomNSC. Sparse models are able to select significant features which mostly contributes to the discrimination function by using built-in variable selection criterias. If a selected classifier is sparse, one may return selected features using getter function selectedGenes.

Let's extract selected genes:

```{r}
ml_selected_genes = selectedGenes(fit.plda2)
```

```{r}
ml_selected_genes
```


Save selected genes in file
```{r}
write.table(ml_selected_genes, file = "data/ml_selected_genes.txt", sep = "\t",
            row.names = FALSE, col.names = FALSE, quote=FALSE)
```



